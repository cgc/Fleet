\hypertarget{index_intro_sec}{}\doxysection{Introduction}\label{index_intro_sec}
Fleet is a C++ library for programming language of thought models. In these models, we specify a grammar of primitive operations which can be composed to form complex hypotheses. These hypotheses are best thought of as programs in a (mental) programming language, and the job of learners is to observe data (typically inputs and outputs of programs) and infer the most likely program to have generated the outputs from the inputs. This is accomplished in Fleet by using a fully-\/\+Bayesian setup, with a prior over programs typically defined thought a Probabilistic Context-\/\+Free \mbox{\hyperlink{class_grammar}{Grammar}} (P\+C\+FG) and a likelihood model that typically says that the output of a program is observed with some noise.

Fleet is most similar to L\+O\+Tlib (\href{https://github.com/piantado/LOTlib3}{\texttt{ https\+://github.\+com/piantado/\+L\+O\+Tlib3}}) but is considerably faster. L\+O\+Tlib converts grammar productions into python expressions which are then evaled in python; this process is flexible and powerful, but slow. Fleet avoids this by implementing a lightweight stack-\/based virtual machine in which programs can be directly evaluated. This is especially advantageous when evaluating stochastic hypotheses (e.\+g. those using \mbox{\hyperlink{_random_8h_a40e7e030d95195c87c566b03fdefe44c}{flip()}} or \mbox{\hyperlink{_random_8h_ac2ea1cac6b4c8cad207512d19abe42d7}{sample()}}) in which multiple execution paths must be evaluated. Fleet stores these multiple execution traces of a single program in a priority queue (sorted by probability) and allows you to rapidly explore the space of execution traces.

Fleet is structured to automatically create this virtual machine and a grammar for programs from just the type specification on primitives\+: a function signature yields a P\+C\+FG because the return type can be thought of as a nonterminal in a P\+C\+FG and its arguments can be thought of as children (right hand side of a P\+C\+FG rule). A P\+C\+FG expanded in this way will yield a correctly-\/typed expression, a program, where each function has arguments of the correct types. Note that in Fleet, these types in the P\+C\+FG must be C++ types (intrinsics, classes, or structs), and two types that are the same are always considered to have the same nonterminal type.

Fleet requires you to define a \mbox{\hyperlink{class_grammar}{Grammar}} which includes {\itshape all} of the nonterminal types that are used anywhere in the grammar. Fleet makes heavy use of template metaprogramming, which prevents you from ever having to explicitly write the grammar itself, but it does implicitly construct a grammar using these operations.

In addition, Fleet has a number of built-\/in operations, which do special things to the virtual machine. These include Builtin\+::\+Flip, which stores multiple execution traces; Builtin\+::\+If which uses short-\/circuit evaluation; Builtin\+::\+Recurse, which handles recursives hypotheses; and Builtin\+::X which provides the argument to the expression.\hypertarget{index_subsec_install}{}\doxysubsection{Installation}\label{index_subsec_install}
Fleet is based on header-\/files, and requires no additional dependencies. Command line arguments are processed in C\+L11.\+hpp, which is included in src/dependencies/.

The easiest way to begin using Fleet is to modify one of the examples. For simple rational-\/rules style inference, try Models/\+Rational\+Rules; for an example using stochastic operations, try Models/\+Formal\+Language\+Theory-\/\+Simple.

Fleet is developed using G\+CC 9 (version $>$8 required) and requires C++-\/17 at present, but this may move to C++-\/20 in the future.\hypertarget{index_subsec_examples}{}\doxysubsection{Examples}\label{index_subsec_examples}
Fleet contains several implemented examples of existing program-\/induction models, including identical or close variants of\+:


\begin{DoxyItemize}
\item The rational rules model (Examples/\+Rational\+Rules)\+: Goodman, N. D., Tenenbaum, J. B., Feldman, J., \& Griffiths, T. L. (2008). A rational analysis of rule‚Äêbased concept learning. Cognitive science, 32(1), 108-\/154.
\item The number model (Examples/\+Number-\/\+Simple)\+: Piantadosi, S. T., Tenenbaum, J. B., \& Goodman, N. D. (2012). Bootstrapping in a language of thought\+: A formal model of numerical concept learning. Cognition, 123(2), 199-\/217.
\item The number game (Examples/\+Number\+Game)\+: Tenenbaum, J. B. (1999). A Bayesian framework for concept learning (Doctoral dissertation, Massachusetts Institute of Technology).
\item Inference of a grammar (Examples/\+Grammar\+Inference-\/$\ast$)\+: Piantadosi, S. T., Tenenbaum, J. B., \& Goodman, N. D. (2016). The logical primitives of thought\+: Empirical foundations for compositional cognitive models. Psychological review, 123(4), 392.
\item Formal language theory learning (Examples/\+Formal\+Language\+Theory-\/$\ast$)\+: Yang \& Piantadosi, forthcoming
\end{DoxyItemize}

as well as several other examples, including sorting and first order logical theories.\hypertarget{index_subsec_style}{}\doxysubsection{Style}\label{index_subsec_style}
Fleet is currently written using a signle-\/file, header-\/only style. This a nonstandard C++ style, but it allows for rapid refactoring and prototyping since declarations and implementations are not in separate files. This style may change in the future because it also leads to higher compilation times.

\textbackslash{}subection subsec\+\_\+inference Inference

Fleet provides a number of simple inference routines to use. Examples of each can be found in Models/\+Sorting


\begin{DoxyItemize}
\item Markov-\/\+Chain Monte-\/\+Carlo
\item M\+C\+MC With Parallel Tempering
\item Monte-\/\+Carlo Tree search
\item Beam search
\item Enumeration (with a fast, fancy implementation)
\end{DoxyItemize}

Generally, we find that \mbox{\hyperlink{class_parallel_tempering}{Parallel\+Tempering}} is the fastest and most effective way to search these spaces of programs. These can be compared in Models/\+Sorting, which has examples of each. ~\newline
\hypertarget{index_tutorial_sec}{}\doxysection{Tutorial}\label{index_tutorial_sec}
To illustrate how Fleet works, let\textquotesingle{}s walk through a simple example\+: the key parts of Formal\+Language\+Theory-\/\+Simple. This is a program induction model which takes a collection of strings and tries to find a stochastic program which can explain the observed strings. For example, it might see data like \{\char`\"{}ab\char`\"{}, \char`\"{}abab\char`\"{}, \char`\"{}ababab\char`\"{}\} and then infer that the language is (ab)$^\wedge$n.

Let\textquotesingle{}s first walk through the grammar definition. Here, we first import \mbox{\hyperlink{_grammar_8h}{Grammar.\+h}} and also \mbox{\hyperlink{_singleton_8h}{Singleton.\+h}}. \mbox{\hyperlink{class_singleton}{Singleton}} is a design pattern that permits only one copy of an object to be constructed, which is used here to ensure that we only have one grammar (and aren\textquotesingle{}t, e.\+g. passing copies of it around). Typical to Fleet\textquotesingle{}s code, we define our own grammar as My\+Grammar. The first two template arguments are the input and output types of the function we are learning (here, strings to strings) and the next two are (variadic) template arguments for all of the nonterminal types used by the grammar (here, strings and bools).


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include <string>}}
\DoxyCodeLine{\textcolor{keyword}{using} S = std::string; \textcolor{comment}{// just for convenience}}
\DoxyCodeLine{ }
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_grammar_8h}{Grammar.h}}"}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_singleton_8h}{Singleton.h}}"}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{class }MyGrammar : \textcolor{keyword}{public} \mbox{\hyperlink{class_grammar}{Grammar}}<S,S,  S,bool>,}
\DoxyCodeLine{                  \textcolor{keyword}{public} \mbox{\hyperlink{class_singleton}{Singleton}}<MyGrammar> \{}
\DoxyCodeLine{\textcolor{keyword}{public}:}
\DoxyCodeLine{    MyGrammar() \{}
\DoxyCodeLine{        \textcolor{comment}{// compute the tail of a string: tail(wxyz) -\/> xyz}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"tail(\%s)"},      +[](S s)      -\/> S \{ \textcolor{keywordflow}{return} (s.empty() ? S(\textcolor{stringliteral}{""}) : s.substr(1,S::npos)); \});}
\DoxyCodeLine{}
\DoxyCodeLine{        \textcolor{comment}{// compute the head of a string: tail(wxyz) -\/> w}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"head(\%s)"},      +[](S s)      -\/> S \{ \textcolor{keywordflow}{return} (s.empty() ? S(\textcolor{stringliteral}{""}) : S(1,s.at(0))); \});}
\DoxyCodeLine{}
\DoxyCodeLine{        \textcolor{comment}{// concatenate strings pair(wx,yz) -\/> wxyz}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"pair(\%s,\%s)"},   +[](S a, S b) -\/> S \{ }
\DoxyCodeLine{            \textcolor{keywordflow}{if}(a.length() + b.length() > MAX\_LENGTH) \textcolor{keywordflow}{throw} \mbox{\hyperlink{class_v_m_s_runtime_error}{VMSRuntimeError}}();}
\DoxyCodeLine{            \textcolor{keywordflow}{else}                                     \textcolor{keywordflow}{return} a+b; }
\DoxyCodeLine{        \});}
\DoxyCodeLine{}
\DoxyCodeLine{        \textcolor{comment}{// the empty string}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"\(\backslash\)u00D8"},        +[]()         -\/> S          \{ \textcolor{keywordflow}{return} S(\textcolor{stringliteral}{""}); \});}
\DoxyCodeLine{          }
\DoxyCodeLine{        \textcolor{comment}{// check if two strings are equal}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"(\%s==\%s)"},      +[](S x, S y) -\/> \textcolor{keywordtype}{bool}       \{ \textcolor{keywordflow}{return} x==y; \});}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{        \textcolor{comment}{// logical operations}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"and(\%s,\%s)"},    Builtins::And<MyGrammar>);}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"or(\%s,\%s)"},     Builtins::Or<MyGrammar>);}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"not(\%s)"},       Builtins::Not<MyGrammar>);}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"if(\%s,\%s,\%s)"},  Builtins::If<MyGrammar,S>);}
\DoxyCodeLine{        }
\DoxyCodeLine{        \textcolor{comment}{// access the argument x to the program (defined to a string type above)}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"x"},             Builtins::X<MyGrammar>);}
\DoxyCodeLine{         }
\DoxyCodeLine{        \textcolor{comment}{// random coin flip (this does something fancy in evaluation)}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"flip()"},        Builtins::Flip<MyGrammar>, 10.0);}
\DoxyCodeLine{         }
\DoxyCodeLine{        \textcolor{comment}{// recursion}}
\DoxyCodeLine{        \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{add}}(\textcolor{stringliteral}{"recurse(\%s)"},   Builtins::Recurse<MyGrammar>);}
\DoxyCodeLine{        }
\DoxyCodeLine{        \textcolor{comment}{// add all the characters in our alphabet (a global variable)}}
\DoxyCodeLine{        \textcolor{keywordflow}{for}(\textcolor{keyword}{const} \textcolor{keywordtype}{char} c : alphabet) \{}
\DoxyCodeLine{            \mbox{\hyperlink{class_grammar_af43a4d8c3c2c7df80a916a5219b7232c}{add\_terminal}}( \mbox{\hyperlink{_strings_8h_a1bb5ec0bee0c8456e3fcc87cc012d055}{Q}}(S(1,c)), S(1,c), 5.0/alphabet.length());}
\DoxyCodeLine{        \}}
\DoxyCodeLine{        }
\DoxyCodeLine{    \}}
\DoxyCodeLine{\};}
\end{DoxyCode}


Then, in the constructor My\+Grammar(), we call the member function \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{Grammar\+::add}} which takes two argumetns\+: a string for how to show the function, and a lambda expression for the function itself. For example, the first function computes the tail of a string (everything except the first character), which shows up in printed programs as \char`\"{}tail(\%s)\char`\"{} (where the s is replaced by the arguments to the function. Note that in Fleet, you must include as many s as there are arugments, meaning that all arguments must be shown.

The second argument to \mbox{\hyperlink{class_grammar_a07ae62095abaf00b3da2fc5065c941bf}{Grammar\+::add}} is a lambda expression which computes and returns the tail of a string. Note that this function, as is generally the case in Fleet, attempts to be relatively fault-\/tolerant rather than throwing exceptions because that\textquotesingle{}s faster. In this case, we just define the tail of an empty string to be the empty string, rather than an error.

The functions for computing the head of a string is defined similarly. The \char`\"{}pair(\%s,\%s)\char`\"{} function is meant to concatenate strings but it actually needs to do a little checking because otherwise it\textquotesingle{}s very easy to write programs which create exponentially long strings. So this function checks if its two argument lengths are above a global variable M\+A\+X\+\_\+\+L\+E\+N\+G\+TH and if so it throws a \mbox{\hyperlink{class_v_m_s_runtime_error}{V\+M\+S\+Runtime\+Error}} (\mbox{\hyperlink{class_virtual_machine_state}{Virtual\+Machine\+State}} error) which is caught and handled internally in Fleet. (Note that in the actual Model code, there is a more complex version of pair which is a bit faster, as it modifies strings stored in the \mbox{\hyperlink{class_virtual_machine_state}{Virtual\+Machine\+State}} stack rather than passing them around, as here).

Then, we have a function which takes no arguments and only returns the empty string. Here, we have given it a Unicode name \textbackslash{}u00\+D8 which will appear/render in most terminals as epsilon (a convention for the empty string in formal language theory).

The \char`\"{}(\%s==\%s)\char`\"{} function computes whether two strings are equal and returns a bool. Note that generally it is helpful to enclose expressions like this in parentheses (i.\+e. \char`\"{}(...)\char`\"{}) because without this, the printed programs can be ambiguous.

We then add built-\/in logical operations. Note that we do {\itshape not} define our own versions of these functions with lambda expressions. We certainly could, but in most computer science applications we want the versions of these functions which involve \href{https://en.wikipedia.org/wiki/Short-circuit_evaluation}{\texttt{ short circuit evaluation}}. For example, and(x,y) won\textquotesingle{}t evaluate y if x is false, which for us ends up with a measurably faster implementation. This is maybe most important though for \char`\"{}if(\%s,\%s,\%s)\char`\"{}, where we only want to evaluate one branch, depending on which way the boolean comes out. The implementation of correct short-\/circuit evaluation depends on some of the internals of Fleet and so it\textquotesingle{}s best to use the built-\/in versions of these functions.

Next, we have a special fucntion \mbox{\hyperlink{namespace_builtins_a25cc5743603132ee78d118a4ca039c68}{Builtins\+::X}} which provides the argument to the function/program we are learning. We defaulty give this the name \char`\"{}x\char`\"{}.

The \char`\"{}flip()\char`\"{} primitive is somewhat fancy (and Buillt-\/in) because it is a stochastic operation, which acts like a coin flip, returning true half of the time and false half of the time (Fleet also a built-\/in biased coin, which takes a coin weight as a parameter). This is fancy because it means that when the program evaluates, there is no longer one single output, but rather a distribution of possible return values. Fleet will hand these back to you as a \mbox{\hyperlink{class_discrete_distribution}{Discrete\+Distribution}} of the output type of the function. Here, we have included 10.\+0 after the \mbox{\hyperlink{namespace_builtins_afc5e30bbc3268c0f88d8540421d453ff}{Builtins\+::\+Flip}}. This is an optional number which can be included for any primitive and it determines the probability in the P\+C\+FG of each each expansion (they are summed and renormalized for each nonterminal type). In this case, we need to upweight the probability of Flip in order to make a proper P\+C\+FG (e.\+g. one where no probabliity is given to infinite trees) because otherwise the and,or,not operatiosn above will lead to infinite trees. The default weight for all operations (when its not specified) is 1.\+0, meaning that 10.\+0 makes it ten times as likely that a bool will expand to a flip than the others. In general, you will want to upweight the terminal rules (those with no children) in order to keep grammars proper.

The next operation is Builtin\+::\+Recurse, which allows a defined function to call itself recursively.

Finally, this grammar adds one terminal rule for each character in the global string \char`\"{}alphabet.\char`\"{} Here, these characters are converted to strings containing a single character because char is not a nonterminal type in the grammar we defined. Here, the entire collection of characters is given an unnonormalized grammar probabiltiy of 5.\+0.

Next, we define a class, My\+Hypothesis, which defines the program hypotheses we are going to be learning. The simplest form for this is to use something of type \mbox{\hyperlink{class_l_o_t_hypothesis}{L\+O\+T\+Hypothesis}}, which defines built-\/in functions for computing the prior, copying, proposing, etc. 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_l_o_t_hypothesis_8h}{LOTHypothesis.h}}"}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{// Declare a hypothesis class}}
\DoxyCodeLine{\textcolor{keyword}{class }MyHypothesis : \textcolor{keyword}{public} \mbox{\hyperlink{class_l_o_t_hypothesis}{LOTHypothesis}}<MyHypothesis,S,S,MyGrammar> \{}
\DoxyCodeLine{\textcolor{keyword}{public}:}
\DoxyCodeLine{    \textcolor{keyword}{using} Super =  \mbox{\hyperlink{class_l_o_t_hypothesis}{LOTHypothesis<MyHypothesis,S,S,MyGrammar>}};}
\DoxyCodeLine{    \textcolor{keyword}{using} Super::Super; \textcolor{comment}{// inherit the constructors}}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{keywordtype}{double} \mbox{\hyperlink{class_l_o_t_hypothesis_aba33afbf945d7eed8c7e07d5a0d31562}{compute\_single\_likelihood}}(\textcolor{keyword}{const} datum\_t\& x)\textcolor{keyword}{ override }\{   }
\DoxyCodeLine{        \textcolor{keyword}{const} \textcolor{keyword}{auto} out = \mbox{\hyperlink{class_callable_af10dbf87425f03b7153f8ec230bf9116}{call}}(x.input, \textcolor{stringliteral}{""}); }
\DoxyCodeLine{        \textcolor{keyword}{const} \textcolor{keyword}{auto} log\_A = log(alphabet.size());}
\DoxyCodeLine{        }
\DoxyCodeLine{        \textcolor{comment}{// Likelihood comes from all of the ways that we can delete from the end and the append to make the observed output. }}
\DoxyCodeLine{        \textcolor{keywordtype}{double} lp = -\/\mbox{\hyperlink{_numerics_8h_af9434aea82baf2f6a5d9b6f9e36db08e}{infinity}};}
\DoxyCodeLine{        \textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\& o : out.values()) \{ \textcolor{comment}{// add up the probability from all of the strings}}
\DoxyCodeLine{            lp = \mbox{\hyperlink{_numerics_8h_a558a1133b9942f8debd559bb9317bca3}{logplusexp}}(lp, o.second + p\_delete\_append<strgamma,strgamma>(o.first, x.output, log\_A));}
\DoxyCodeLine{        \}}
\DoxyCodeLine{        \textcolor{keywordflow}{return} lp;}
\DoxyCodeLine{    \}}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{keywordtype}{void} \mbox{\hyperlink{class_bayesable_a87d5d9481d6a72b017e44b175071fa5e}{print}}(std::string prefix=\textcolor{stringliteral}{""})\textcolor{keyword}{ override }\{}
\DoxyCodeLine{        prefix = prefix+\textcolor{stringliteral}{"\#\(\backslash\)n\#"} +  this-\/>\mbox{\hyperlink{class_callable_af10dbf87425f03b7153f8ec230bf9116}{call}}(\textcolor{stringliteral}{""}, \textcolor{stringliteral}{"<err>"}).string() + \textcolor{stringliteral}{"\(\backslash\)n"};}
\DoxyCodeLine{        \mbox{\hyperlink{namespace_fleet_args_acf2f80bb2810cea3b3eeef0c4b0edf03}{Super::print}}(prefix); }
\DoxyCodeLine{    \}}
\DoxyCodeLine{\};  }
\end{DoxyCode}


Here, My\+Hypothesis uses the \href{https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern}{\texttt{ curiously recurring template pattern}} to pass itself, so that it knows how to make copies of itself. The other template arguments ar ethe input and output types of its function (string to string) and the \mbox{\hyperlink{class_grammar}{Grammar}} type it uses (My\+Grammar). Then, we inherit \mbox{\hyperlink{class_l_o_t_hypothesis}{L\+O\+T\+Hypothesis}}\textquotesingle{} constructors (via Super\+::\+Super). Primarily, what we have to define here is the compute\+\_\+single\+\_\+likelihood function, which defines the likelihood of a single observed data string. To compute this, we first call the function (\mbox{\hyperlink{class_callable_af10dbf87425f03b7153f8ec230bf9116}{L\+O\+T\+Hypothesis\+::call}}) on the observed input. This returns a Discrete\+Distribution$<$std\+::string$>$ of outputs. Note that the outputs which return errors are mapped to the second argument to call, in this case the empty stirng. Then, we loop over outputs and add up the likelihood of the observed output x.\+ouput given the program output o.\+first weighted by the program\textquotesingle{}s probability of that output o.\+second. This part uses p\+\_\+delete\+\_\+append, which is a string edit likelihood that assigns nonzero probability of any string being corrupted to any other. It computes this by imagining that the output string was corrupted by noise that deleted from the end of the string, and then appended, where both operations take place with geometric probabilities. These probabilities are passed as template arguments since that lets us compute some key pieces of this at compile time, yielding much faster code (this is a hot part of any string-\/based inference model).

We also define a print function which allows us to say how we print out the program hypothesis. Defaultly, any \mbox{\hyperlink{class_l_o_t_hypothesis}{L\+O\+T\+Hypothesis}} will just print (using the format strings like \char`\"{}pair(\%s,\%s)\char`\"{} above) but here we would like to also show the distribution of outputs when we print, so this prints a line with a \mbox{\hyperlink{class_callable_af10dbf87425f03b7153f8ec230bf9116}{L\+O\+T\+Hypothesis\+::call}} (which is a \mbox{\hyperlink{class_discrete_distribution}{Discrete\+Distribution}}) and then a .string() call on the returned value.

The last piece of code is the actual running\+: Here, we define a Fleet object, which basically manages the input/output. We tell it that we want to be able to specify the alphabet and a string for the data (comma separated strings) on the command line. We create a grammar and note that this must happend after Fleet.\+initialize, since creating the grammar object needs to know the alphabet. Then, we create a \mbox{\hyperlink{class_top_n}{TopN}} object to store the top hypotheses we have found. This is basically a sorted collection of the best hypotheses found so far. We set a few control aspects of our Virtual\+Machine (e.\+g. how many steps we should run each program for, how many outputs we will consider for each program). We convert our datastr (specified on the command line) to type My\+Hypothesis\+::data\+\_\+t, and do a check on whether the data contains anything not in the alphabet.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "Top.h"}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_parallel_tempering_8h}{ParallelTempering.h}}"}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_fleet_8h}{Fleet.h}}"} }
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_builtins_8h}{Builtins.h}}"}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "\mbox{\hyperlink{_v_m_s_runtime_error_8h}{VMSRuntimeError.h}}"}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{int} main(\textcolor{keywordtype}{int} argc, \textcolor{keywordtype}{char}** argv)\{ }
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// define a Fleet object}}
\DoxyCodeLine{    Fleet fleet(\textcolor{stringliteral}{"A simple, one-\/factor formal language learner"});}
\DoxyCodeLine{    fleet.add\_option(\textcolor{stringliteral}{"-\/a,-\/-\/alphabet"}, alphabet, \textcolor{stringliteral}{"Alphabet we will use"});    \textcolor{comment}{// add my own args}}
\DoxyCodeLine{    fleet.add\_option(\textcolor{stringliteral}{"-\/d,-\/-\/data"},     datastr, \textcolor{stringliteral}{"Comma separated list of input data strings"});   }
\DoxyCodeLine{    fleet.initialize(argc, argv);}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// create a grammar}}
\DoxyCodeLine{    MyGrammar grammar;}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// top stores the top hypotheses we have found}}
\DoxyCodeLine{    \mbox{\hyperlink{class_top_n}{TopN<MyHypothesis>}} top;}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// set these global variables to make VMS a little faster, less accurate}}
\DoxyCodeLine{    \mbox{\hyperlink{struct_virtual_machine_control_a06c58af00696ae605591479ca1d7ccc9}{VirtualMachineControl::MAX\_STEPS}} = 256;}
\DoxyCodeLine{    \mbox{\hyperlink{struct_virtual_machine_control_a0f7fc535fbcbc25b9be7a377aad1b283}{VirtualMachineControl::MAX\_OUTPUTS}} = 256;}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// mydata stores the data for the inference model}}
\DoxyCodeLine{    \textcolor{keyword}{auto} mydata = string\_to<MyHypothesis::data\_t>(datastr);}
\DoxyCodeLine{        }
\DoxyCodeLine{    assert(not \mbox{\hyperlink{_strings_8h_a9e236149ce0f5aa64d809fcca5cf1c27}{contains}}(alphabet, \textcolor{stringliteral}{":"}));\textcolor{comment}{// can't have these or else our string\_to doesn't work}}
\DoxyCodeLine{    assert(not \mbox{\hyperlink{_strings_8h_a9e236149ce0f5aa64d809fcca5cf1c27}{contains}}(alphabet, \textcolor{stringliteral}{","}));}
\DoxyCodeLine{    \textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\& di : mydata) \{}
\DoxyCodeLine{        \textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\& c: di.output) \{}
\DoxyCodeLine{            assert(\mbox{\hyperlink{_strings_8h_a9e236149ce0f5aa64d809fcca5cf1c27}{contains}}(alphabet, c));}
\DoxyCodeLine{        \}}
\DoxyCodeLine{    \}}
\DoxyCodeLine{    }
\DoxyCodeLine{    \textcolor{comment}{// Run}}
\DoxyCodeLine{    top.\mbox{\hyperlink{class_top_n_a90472852f225e5b851446e4b7ced4856}{print\_best}} = \textcolor{keyword}{true}; \textcolor{comment}{// print out each best hypothesis you find}}
\DoxyCodeLine{    \textcolor{keyword}{auto} h0 = MyHypothesis::make(\&grammar);}
\DoxyCodeLine{    \mbox{\hyperlink{class_m_c_m_c_chain}{MCMCChain}} c(h0, \&mydata, top);}
\DoxyCodeLine{    \textcolor{comment}{//c.temperature = 1.0; // if you want to change the temperature -\/-\/ note that lower temperatures tend to be much slower!}}
\DoxyCodeLine{    c.run(\mbox{\hyperlink{struct_control}{Control}}());}
\DoxyCodeLine{}
\DoxyCodeLine{    \textcolor{comment}{// print the hypotheses we found}}
\DoxyCodeLine{    top.\mbox{\hyperlink{class_top_n_a5064ec2e4e9f1defdc3cb62d9b7ddd00}{print}}();}
\DoxyCodeLine{\}}
\end{DoxyCode}


And finally, we run the actual model. We make an initial hypothesis using My\+Hypothesis\+::make (which needs to know the grammar), we make an \mbox{\hyperlink{class_m_c_m_c_chain}{M\+C\+M\+C\+Chain}}, which takes an initial hypothesis, data, and a callback (which top functions as), and then we run it. The c.\+run function takes a \mbox{\hyperlink{struct_control}{Control}} object, which basically specifies the number of steps to run or amount of time to run, the amount of thinning, etc. When \mbox{\hyperlink{struct_control}{Control()}} is called with no arguments, it gets its arguments from the command line. This means that automatically, we can give a commandline arugment like \char`\"{}-\/-\/time=30s\char`\"{} and this fleet program will run for 30 seconds (times can be in days (d), hours (h), minutes (m), seconds (s) or miliseconds (q).

Note that there are several inference schemes, and most (including the examples in Models) use \mbox{\hyperlink{class_parallel_tempering}{Parallel\+Tempering}}, which seems to work best.

As the chain runs, it will place hypotheses into \char`\"{}top\char`\"{} (which defaultly reads the command line argument (e.\+g. \char`\"{}-\/-\/top=25\char`\"{}) to figure out how many of the best hypotheses to store. At the end of this code, we call \mbox{\hyperlink{namespace_fleet_args_acf2f80bb2810cea3b3eeef0c4b0edf03}{top.\+print()}}, which will call the .\mbox{\hyperlink{namespace_fleet_args_acf2f80bb2810cea3b3eeef0c4b0edf03}{print()}} function on everything in top and show them sorted by posterior probability score (remember above we overwrote My\+Hypothesis\+::print in order to have it run the program).

At the end of this program, as the Fleet object is destroyed, it will print out some statistics about the total number of M\+C\+MC steps, total runtime, etc. (This can be turned off by setting \mbox{\hyperlink{namespace_fleet_args_acb1d87c67d763d1edf83ebe1377c948e}{Fleet\+Args\+::print\+\_\+header}}=false). 